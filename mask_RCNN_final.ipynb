{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHnVupBBn9eR"
      },
      "source": [
        "# Detectron2 Beginner's Tutorial\n",
        "\n",
        "<img src=\"https://dl.fbaipublicfiles.com/detectron2/Detectron2-Logo-Horz.png\" width=\"500\">\n",
        "\n",
        "Welcome to detectron2! This is the official colab tutorial of detectron2. Here, we will go through some basics usage of detectron2, including the following:\n",
        "* Run inference on images or videos, with an existing detectron2 model\n",
        "* Train a detectron2 model on a new dataset\n",
        "\n",
        "You can make a copy of this tutorial by \"File -> Open in playground mode\" and make changes there. __DO NOT__ request access to this tutorial.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vM54r6jlKTII"
      },
      "source": [
        "# Install detectron2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FsePPpwZSmqt"
      },
      "outputs": [],
      "source": [
        "!pip install pyyaml==5.1\n",
        "# This is the current pytorch version on Colab. Uncomment this if Colab changes its pytorch version\n",
        "!pip install torch==1.9.0+cu102 torchvision==0.10.0+cu102 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "\n",
        "# Install detectron2 that matches the above pytorch version\n",
        "# See https://detectron2.readthedocs.io/tutorials/install.html for instructions\n",
        "!pip install detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/cu102/torch1.9/index.html\n",
        "exit(0)  # After installation, you need to \"restart runtime\" in Colab. This line can also restart runtime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o1BN8Ysj7wN3"
      },
      "outputs": [],
      "source": [
        "!pip install torch==1.9.0+cu102 torchvision==0.10.0+cu102 -f https://download.pytorch.org/whl/torch_stable.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9_FzH13EjseR",
        "outputId": "e482a2da-49a8-4368-fa02-8ee7af75a28e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.9.0+cu102 True\n"
          ]
        }
      ],
      "source": [
        "# check pytorch installation: \n",
        "import torch, torchvision\n",
        "print(torch.__version__, torch.cuda.is_available())\n",
        "assert torch.__version__.startswith(\"1.9\")   # please manually install torch 1.9 if Colab changes its default version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "6oKpHZ9N-B1c",
        "outputId": "749ccedd-751d-4141-fcfc-0cdef9e6abb7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.9.0+cu102'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "torch.__version__"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "JlLHREIqre_K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ZyAvNCJMmvFF"
      },
      "outputs": [],
      "source": [
        "# Some basic setup:\n",
        "# Setup detectron2 logger\n",
        "import detectron2\n",
        "from detectron2.utils.logger import setup_logger\n",
        "setup_logger()\n",
        "\n",
        "# import some common libraries\n",
        "import numpy as np\n",
        "import os, json, cv2, random\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "# import some common detectron2 utilities\n",
        "from detectron2 import model_zoo\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "from detectron2.data import MetadataCatalog, DatasetCatalog"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2bjrfb2LDeo"
      },
      "source": [
        "# Train on a custom dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tjbUIhSxUdm_"
      },
      "source": [
        "In this section, we show how to train an existing detectron2 model on a custom dataset in a new format.\n",
        "\n",
        "We use [the balloon segmentation dataset](https://github.com/matterport/Mask_RCNN/tree/master/samples/balloon)\n",
        "which only has one class: balloon.\n",
        "We'll train a balloon segmentation model from an existing model pre-trained on COCO dataset, available in detectron2's model zoo.\n",
        "\n",
        "Note that COCO dataset does not have the \"balloon\" category. We'll be able to recognize this new class in a few minutes.\n",
        "\n",
        "## Prepare the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scKK6HqR8GVs"
      },
      "source": [
        "### TACO dataset\n",
        "#### - either download it from zip\n",
        "#### - or mount from google drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1uUDwNzWDano"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/visriv/TACO.git\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "dataset_path = \"/content/gdrive/MyDrive/data\"\n",
        "!python ./TACO/detector/split_dataset.py --dataset_dir ./gdrive/MyDrive/data --nr_trials 1\n",
        "\n",
        "\n",
        "'''\n",
        "Need to check if paths are correct/stitched correctly\n",
        "\n",
        "zip_path = './data/taco_datset.zip'\n",
        "!curl https://zenodo.org/record/3587843/files/TACO.zip --output $zip_path\n",
        "import zipfile\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(zip_path.replace('.zip', '/'))\n",
        "dataset_path = './data/taco_datset/TACO/data/'\n",
        "'''\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_K8cT86cgDaB"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "UR1V3tiFsu_9"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GUd_Skj0t3P9"
      },
      "outputs": [],
      "source": [
        "from detectron2.structures import BoxMode\n",
        "import pandas as pd\n",
        "import json\n",
        "def myconverter(obj):\n",
        "    if isinstance(obj, np.integer):\n",
        "        return int(obj)\n",
        "    elif isinstance(obj, np.floating):\n",
        "        return float(obj)\n",
        "    elif isinstance(obj, np.ndarray):\n",
        "        return obj.tolist()\n",
        "    elif isinstance(obj, datetime.datetime):\n",
        "        return obj.__str__()\n",
        "\n",
        "def get_taco_dicts(input_json_path,\n",
        "                   filter_type = 'default',\n",
        "                   n_top_categories=28,\n",
        "                   label_transfer = None):\n",
        "    # load data using Python JSON module\n",
        "    dataset_path = '/'.join(input_json_path.split('/')[0:-1])\n",
        "    with open(input_json_path,'r') as json_in:\n",
        "      data_json = json.loads(json_in.read())\n",
        "      global json_df_final_category\n",
        "\n",
        "              \n",
        "      json_df_image = pd.json_normalize(data_json, record_path =['images'])\n",
        "      json_df_anno = pd.json_normalize(data_json, record_path =['annotations'])\n",
        "      json_df_anno_u = json_df_anno.rename(columns = {'id': 'record_id', 'image_id': 'id'}, inplace = False)\n",
        "      json_df_anno_u['segmentation'] = json_df_anno_u['segmentation'].apply(lambda x: x[0])\n",
        "\n",
        "      json_df_cat = pd.json_normalize(data_json, record_path =['categories'])\n",
        "      json_df_cat_new = json_df_cat.rename(columns= {'id':'category_id'})\n",
        "      json_df_image = pd.merge(json_df_image, json_df_anno_u, on=\"id\")\n",
        "      \n",
        "      json_df_final_category = pd.merge(json_df_image, json_df_cat_new, on=\"category_id\")\n",
        "      json_df_final_category.sort_values('id', inplace=True)\n",
        "      #json_df_final_category = json_df_final_category.set_index('id')\n",
        "      json_df_final_category['image_count']=json_df_final_category.groupby('supercategory')['id'].transform('count')\n",
        "      json_df_final_category['category_rank'] = json_df_final_category['image_count'].rank(method='dense', ascending=False).astype('int32')\n",
        "      json_df_final_category = json_df_final_category.astype({'id':'int32', \n",
        "                                                        'width':'int32', \n",
        "                                                        'height':'int32', \n",
        "                                                        'record_id':'int32', \n",
        "                                                        'category_id':'int32', \n",
        "                                                        'image_count':'int32',\n",
        "                                                        'iscrowd':'int32'})\n",
        "      print('total image df count', json_df_final_category.shape[0],\n",
        "            'total anno df count', json_df_final_category.shape[0], \n",
        "            'total joined df count', json_df_final_category.shape[0])\n",
        "      \n",
        "      if filter_type == 'label_transfer':\n",
        "        json_df_final_category = json_df_final_category[json_df_final_category['supercategory'].isin(list(label_transfer.keys()))]\n",
        "        json_df_final_category['super_id'] = json_df_final_category['supercategory'].apply(lambda x: label_transfer[x])\n",
        "      else:\n",
        "        json_df_final_category = json_df_final_category[json_df_final_category['category_rank'] <= n_top_categories]\n",
        "        json_df_final_category['super_id'] = json_df_final_category.groupby(['supercategory']).ngroup().astype('int32')\n",
        "      \n",
        "      print('selected image df count', json_df_final_category.shape[0],\n",
        "            'selected anno df count', json_df_final_category.shape[0], \n",
        "            'selected joined df count', json_df_final_category.shape[0])\n",
        "      print(json_df_final_category)\n",
        "\n",
        "    \n",
        "\n",
        "    dataset_dicts = []\n",
        "    idx = 0\n",
        "    while idx < len(json_df_final_category.index):\n",
        "        row = json_df_final_category.iloc[idx]\n",
        "        record = {}\n",
        "        record[\"file_name\"] = dataset_path + '/' + row['file_name']\n",
        "        record[\"image_id\"] = myconverter(row['id'])\n",
        "        record[\"height\"] = row['height']\n",
        "        record[\"width\"] = row['width']\n",
        "        \n",
        "        i = 0\n",
        "        objs = []\n",
        "        #print(type(row['id']), type(json_df_final_category.iloc[idx+i]['id']))\n",
        "        #print(type(row['id']) ==  type(json_df_final_category.iloc[idx+i]['id']))\n",
        "        #print(idx+i < len(json_df_final_category.index) and  (row['id'] == json_df_final_category.iloc[idx+i]['id']))\n",
        "        while (idx+i < len(json_df_final_category.index) and row['id'] == json_df_final_category.iloc[idx+i]['id']):\n",
        "          obj = {\n",
        "              \"bbox\": json_df_final_category.iloc[idx+i]['bbox'],\n",
        "              \"bbox_mode\": BoxMode.XYWH_ABS,\n",
        "              \"segmentation\":[json_df_final_category.iloc[idx+i]['segmentation']],\n",
        "              \"category_id\": json_df_final_category.iloc[idx+i]['super_id']\n",
        "              }\n",
        "          objs.append(obj)\n",
        "          i = i+1\n",
        "          #print('idx, i: ', idx, i)\n",
        "          #print('appended annotations of length:', len(objs))\n",
        "\n",
        "        record[\"annotations\"] = objs\n",
        "        dataset_dicts.append(record)\n",
        "        idx = idx+i+1\n",
        "        #print(idx)\n",
        "    return dataset_dicts\n",
        "\n",
        "'''\n",
        "label_transfer = {'Plastic bag & wrapper':0,\n",
        "                  'Cigarette':1,\n",
        "                  'Unlabeled litter' :2,\n",
        "                  'Aluminium foil':3,\n",
        "                  'Battery':4,\n",
        "                  'Blister pack':5,\n",
        "                  'Bottle':6,\n",
        "                  'Can':7,\n",
        "                  'Carton':8,\n",
        "                  'Broken glass':9,\n",
        "                  'Bottle cap': 10, 'Cup':11,\n",
        "                  'Straw':12,\n",
        "                  'Other plastic':13,\n",
        "                  'Paper':14,\n",
        "                  'Paper bag':15,\n",
        "                  'Styrofoam piece':16,\n",
        "                  'Pop tab': 17,\n",
        "                  'Lid':18,\n",
        "                  'Plastic container':19}\n",
        "'''\n",
        "#for drivable (0)/non-drivable (1)\n",
        "label_transfer = {'Plastic bag & wrapper':0, 'Cigarette':0, 'Unlabeled litter' :1, 'Aluminium foil':0, 'Battery':1, \n",
        "                 'Blister pack':0, 'Bottle':1, 'Can':1, 'Carton':0, 'Broken glass':1, 'Bottle cap': 0, 'Cup':0, 'Straw':0,\n",
        "                  'Other plastic':1, 'Paper':0, 'Paper bag':0, 'Styrofoam piece':0, 'Pop tab': 0, 'Lid':0, 'Plastic container':0, 'Glass jar':1}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "dataset_dicts = get_taco_dicts(dataset_path + \"/annotations_0_train.json\", \n",
        "                               filter_type = 'label_transfer',\n",
        "                               label_transfer = label_transfer)\n",
        "run_id = 19\n",
        "\n",
        "         "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "K1TSbJ7ryctn"
      },
      "outputs": [],
      "source": [
        "for d in [\"train\" + str(run_id), \"val\"+ str(run_id)]:\n",
        "    DatasetCatalog.register(\"taco_\" + d, \n",
        "                            lambda d=d: get_taco_dicts(dataset_path + \"/annotations_0_\" + d[:-1*len(str(run_id))] + '.json', \n",
        "                            filter_type = 'label_transfer',\n",
        "                            label_transfer = label_transfer))\n",
        "    #supercategory_names = list(set(json_df_final_category[\"supercategory\"]))\n",
        "    supercategory_names = ['Driveable', 'Non-driveable']\n",
        "    #supercategory_names.sort()\n",
        "    MetadataCatalog.get(\"taco_\" + d).set(thing_classes=supercategory_names)\n",
        "\n",
        "\n",
        "taco_metadata = MetadataCatalog.get(\"taco_train\" + str(run_id))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "aihys1EbqI7R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vLfCxcmwLevH"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1iU47W2ez7r6"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ljbWTX0Wi8E"
      },
      "source": [
        "To verify the data loading is correct, let's visualize the annotations of randomly selected samples in the training set:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UkNbUzUOLYf0"
      },
      "outputs": [],
      "source": [
        "dataset_dicts = get_taco_dicts(dataset_path + \"/annotations_0_train.json\", filter_type = 'label_transfer', label_transfer = label_transfer)\n",
        "from detectron2.utils.visualizer import ColorMode\n",
        "\n",
        "for d in random.sample(dataset_dicts, 3):\n",
        "    print(d, '\\n')\n",
        "    img = cv2.imread(d[\"file_name\"])\n",
        "    visualizer = Visualizer(img[:, :, ::-1], scale=0.18,instance_mode=ColorMode.IMAGE_BW)\n",
        "    out = visualizer.draw_dataset_dict(d)\n",
        "    cv2_imshow(out.get_image()[:, :, ::-1])\n",
        "    #print(json_df_final_category.loc[d['annotations'][0]['category_id']])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2w43r1VB4E5Z"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8K-7YSEC4XKj"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wlqXIXXhW8dA"
      },
      "source": [
        "## Train!\n",
        "\n",
        "Now, let's fine-tune a COCO-pretrained R50-FPN Mask R-CNN model on the taco dataset. It takes ~2 minutes to train 300 iterations on a P100 GPU.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7unkuuiqLdqd"
      },
      "outputs": [],
      "source": [
        "from detectron2.engine import DefaultTrainer\n",
        "model_prefix = \"COCO-InstanceSegmentation\" #\"COCO-Detection\" check on detectron2 github\n",
        "model_name = 'mask_rcnn_R_50_FPN_3x'  # 'faster_rcnn_R_50_FPN_3x' \n",
        "suffix = '_taco2'\n",
        "cfg = get_cfg()\n",
        "cfg.merge_from_file(model_zoo.get_config_file(model_prefix + \"/\" + model_name + \".yaml\"))\n",
        "cfg.DATASETS.TRAIN = (\"taco_train\" + str(run_id))\n",
        "cfg.DATASETS.TEST = ()\n",
        "cfg.DATALOADER.NUM_WORKERS = 2\n",
        "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(model_prefix + \"/\" + model_name + \".yaml\")  # Let training initialize from model zoo\n",
        "cfg.SOLVER.IMS_PER_BATCH = 2\n",
        "cfg.SOLVER.BASE_LR = 0.00025  # pick a good LR\n",
        "cfg.SOLVER.MAX_ITER = 5000  # 300 iterations seems good enough for this toy dataset; you will need to train longer for a practical dataset\n",
        "cfg.SOLVER.STEPS = []        # do not decay learning rate\n",
        "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128   # faster, and good enough for this toy dataset (default: 512)\n",
        "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 2  # max 28 class (supercategories). (see https://detectron2.readthedocs.io/tutorials/datasets.html#update-the-config-for-new-datasets)\n",
        "cfg.OUTPUT_DIR = './output/' + model_name + suffix\n",
        "# NOTE: this config means the number of classes, but a few popular unofficial tutorials incorrect uses num_classes+1 here.\n",
        "\n",
        "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
        "trainer = DefaultTrainer(cfg) \n",
        "trainer.resume_or_load(resume=False)\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ue4wXX3JgvZg"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hBXeH8UXFcqU"
      },
      "outputs": [],
      "source": [
        "# Look at training curves in tensorboard:\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir {cfg.OUTPUT_DIR}\n",
        "%reload_ext tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AEuPXH9cOLrl"
      },
      "outputs": [],
      "source": [
        "!tensorboard dev upload --logdir './output' --name \"taco\" --description \"finetune pretrained faster/mask rcnn on taco_x dataset\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5VVV6Q0u8y4m"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dTxbYADD8y2P"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "BdyaX5aphSA9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "M3lfm1kuocsU"
      },
      "outputs": [],
      "source": [
        "#save to output path in google drive\n",
        "os.makedirs('gdrive/MyDrive/trash_detection/results/' + suffix + '/', exist_ok=True)\n",
        "%cp -r {cfg.OUTPUT_DIR} gdrive/MyDrive/trash_detection/results/$suffix/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0e4vdDIOXyxF"
      },
      "source": [
        "## Inference & evaluation using the trained model\n",
        "Now, let's run inference with the trained model on the taco validation dataset. First, let's create a predictor using the model we just trained:\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWq1XHfDWiXO"
      },
      "source": [
        "Then, we randomly select several samples to visualize the prediction results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "35vnTmoQvDBA"
      },
      "outputs": [],
      "source": [
        "print(outputs[\"instances\"].pred_classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eRzLfquyITqh"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "temp\n",
        "to upload and restore the  checkpoint\n",
        "'''\n",
        "from detectron2.engine import DefaultTrainer\n",
        "\n",
        "cfg = get_cfg()\n",
        "cfg.merge_from_file(model_zoo.get_config_file(model_prefix + \"/\" + model_name + \".yaml\"))\n",
        "cfg.DATASETS.TRAIN = (\"taco_train\" + str(run_id))\n",
        "cfg.DATASETS.TEST = ()\n",
        "cfg.DATALOADER.NUM_WORKERS = 2\n",
        "#cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\")  # Let training initialize from model zoo\n",
        "cfg.SOLVER.IMS_PER_BATCH = 2\n",
        "cfg.SOLVER.BASE_LR = 0.00025  # pick a good LR\n",
        "cfg.SOLVER.MAX_ITER = 5000  # 300 iterations seems good enough for this toy dataset; you will need to train longer for a practical dataset\n",
        "cfg.SOLVER.STEPS = []        # do not decay learning rate\n",
        "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128   # faster, and good enough for this toy dataset (default: 512)\n",
        "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 20  # max 28 class (supercategories). (see https://detectron2.readthedocs.io/tutorials/datasets.html#update-the-config-for-new-datasets)\n",
        "# NOTE: this config means the number of classes, but a few popular unofficial tutorials incorrect uses num_classes+1 here.\n",
        "\n",
        "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8BJCmStYZeUs"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eYizn217YyPA"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U5LhISJqWXgM"
      },
      "outputs": [],
      "source": [
        "# Inference should use the config with parameters that are used in training\n",
        "# cfg now already contains everything we've set previously. We changed it a little bit for inference:\n",
        "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")  # path to the model we just trained\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.50   # set a custom testing threshold\n",
        "predictor = DefaultPredictor(cfg)\n",
        "\n",
        "from detectron2.utils.visualizer import ColorMode\n",
        "import time\n",
        "\n",
        "dataset_dicts_val = get_taco_dicts(dataset_path + \"/annotations_0_test.json\", filter_type = 'label_transfer', label_transfer = label_transfer)\n",
        "os.makedirs(\"gdrive/MyDrive/trash_detection/Inference/\" + suffix + \"/\" + model_name + \"/\", exist_ok = True)\n",
        "for d in dataset_dicts_val:    \n",
        "    im = cv2.imread(d[\"file_name\"])\n",
        "    #record start time of inference\n",
        "    start_time = time.time()\n",
        "\n",
        "    outputs = predictor(im)  # format is documented at https://detectron2.readthedocs.io/tutorials/models.html#model-output-format\n",
        "    \n",
        "    print(\"inference time\", time.time() - start_time)\n",
        "    \n",
        "    '''\n",
        "    out_classes = list(outputs[\"instances\"].pred_classes.to(\"cpu\").numpy())\n",
        "    true_classes = [d['annotations'][i]['category_id'] for i in range(len(d['annotations']))]\n",
        "\n",
        "    common_pred = list(set(out_classes) & set(true_classes))\n",
        "    '''\n",
        "    v = Visualizer(im[:, :, ::-1], metadata=taco_metadata, scale=0.5\n",
        "                     # remove the colors of unsegmented pixels. This option is only available for segmentation models\n",
        "    )\n",
        "    out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
        "    out_image = out.get_image()[:, :, ::-1]\n",
        "    #cv2_imshow(out_image)\n",
        "    img_file_name = d[\"file_name\"].split('/')[-1]\n",
        "    cv2.imwrite(\"gdrive/MyDrive/trash_detection/Inference/\" + suffix + \"/\" + model_name + \"/\" + img_file_name + \".jpg\", out_image)\n",
        "    print(d[\"file_name\"])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "xnXvcZSToFLm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "5s3BZfp3odpa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kblA1IyFvWbT"
      },
      "source": [
        "We can also evaluate its performance using AP metric implemented in COCO API.\n",
        "This gives an AP of ~70. Not bad!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h9tECBQCvMv3"
      },
      "outputs": [],
      "source": [
        "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
        "from detectron2.data import build_detection_test_loader\n",
        "evaluator = COCOEvaluator(\"taco_val\" + str(run_id), output_dir=cfg.OUTPUT_DIR)\n",
        "val_loader = build_detection_test_loader(cfg, \"taco_val\" + str(run_id))\n",
        "print(inference_on_dataset(predictor.model, val_loader, evaluator))\n",
        "# another equivalent way to evaluate the model is to use `trainer.test`"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "zItinOhslBiz"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import fvcore.nn.parameter_count\n",
        "from detectron2.modeling import build_model\n",
        "model = build_model(cfg)  # returns a torch.nn.Module\n",
        "cnt = fvcore.nn.parameter_count(model)\n",
        "print(cnt)"
      ],
      "metadata": {
        "id": "UOug1LGZkvJE"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "-GcfyNhulLjF"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "mask RCNN final.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}